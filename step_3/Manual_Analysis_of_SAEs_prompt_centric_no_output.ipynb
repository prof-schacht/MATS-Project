{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "# Import SAE \n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "from sae_lens import TrainingSAE\n",
    "## Loading SAE\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", device=\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAES at ones \n",
    "saes = []\n",
    "checkpoint_dir = \"/proj/mats_checkpoints\"\n",
    "for run_folder in os.listdir(checkpoint_dir):\n",
    "    run_path = os.path.join(checkpoint_dir, run_folder)\n",
    "    if os.path.isdir(run_path):\n",
    "        final_folder = os.path.join(run_path, \"final_163840000\")\n",
    "        if os.path.exists(final_folder):\n",
    "            sae = TrainingSAE.load_from_pretrained(path=final_folder, device=\"cuda:1\")\n",
    "            saes.append(sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sae_data = []\n",
    "for sae in saes:\n",
    "    sae_data.append({\n",
    "        'hook_name': sae.cfg.hook_name,\n",
    "        'hook_layer': sae.cfg.hook_layer,\n",
    "        'model_name': sae.cfg.model_name,\n",
    "        'd_sae': sae.cfg.d_sae,\n",
    "        'context_size': sae.cfg.context_size,\n",
    "        'd_in': sae.cfg.d_in,\n",
    "        'dataset_path': sae.cfg.dataset_path\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(sae_data)\n",
    "df = df.sort_values('hook_layer')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: Getting Features Using SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "prompt = \"An 80 m long cable is suspended from the top of two masts, both of which are 50 m above the ground. What is the distance between the two masts to one decimal place if the center of the cable is 10 m above the ground?\"\n",
    "answer = \"0\"\n",
    "\n",
    "test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(prompt, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sae in saes:\n",
    "    sae.use_error_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooked SAE Transformer will enable us to get the feature activations from the SAE\n",
    "\n",
    "_, cache = model.run_with_cache_with_saes(prompt, saes=saes)\n",
    "print([(k, v.shape) for k,v in cache.items() if \"sae\" in k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Sort SAEs by hook_layer in ascending order\n",
    "sorted_saes = sorted(saes, key=lambda x: x.cfg.hook_layer)\n",
    "\n",
    "# Create a subplot for each SAE\n",
    "fig = sp.make_subplots(rows=len(sorted_saes), cols=1, subplot_titles=[f\"SAE {sae.cfg.hook_layer}\" for sae in sorted_saes])\n",
    "\n",
    "for i, sae in enumerate(sorted_saes):\n",
    "    # Get the cache key for this SAE\n",
    "    cache_key = f'blocks.{sae.cfg.hook_layer}.hook_resid_post.hook_sae_acts_post'\n",
    "    \n",
    "    # Create a line trace for this SAE\n",
    "    trace = go.Scatter(\n",
    "        y=cache[cache_key][0, -1, :].cpu().numpy(),\n",
    "        mode='lines',\n",
    "        name=f'SAE {sae.cfg.hook_layer}',\n",
    "        hovertemplate='Feature: %{x}<br>Activation: %{y}'\n",
    "    )\n",
    "    \n",
    "    # Add the trace to the subplot\n",
    "    fig.add_trace(trace, row=i+1, col=1)\n",
    "\n",
    "    # Update x and y axis labels\n",
    "    fig.update_xaxes(title_text=\"Feature\", row=i+1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Activation\", row=i+1, col=1)\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    height=300*len(sorted_saes),  # Adjust height based on number of SAEs\n",
    "    title_text=f\"Feature activations at the final token position for each SAE (ordered by hook layer)<br>Using the prompt: '{prompt}'\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_value = cache['blocks.25.hook_resid_post.hook_sae_acts_post'][0, -1, :].cpu().numpy()\n",
    "max_index = np.argmax(max_value)\n",
    "print(f\"Index of maximal value: {max_index}\")\n",
    "\n",
    "print(max_value[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's print the top 5 features and how much they fired\n",
    "vals, inds = torch.topk(cache['blocks.25.hook_resid_post.hook_sae_acts_post'][0, -1, :], 5)\n",
    "for val, ind in zip(vals, inds):\n",
    "    print(f\"Feature {ind} fired {val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The contrastive Pairs Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "prompt = \"In the beginning, God created the cat and the\"\n",
    "answer = \"earth\"\n",
    "\n",
    "# here we see that removing the word \"Heavens\" is very effective at making the model no longer predict \"earth\".\n",
    "# instead the model predicts a bunch of different animals.\n",
    "# Can we work out which features fire differently which might explain this? (This is a toy example not meant to be super interesting)\n",
    "test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"In the bgeinning, God created the heavens and the\", \"In the beginning, God created the cat and the\"]\n",
    "_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
    "print([(k, v.shape) for k,v in cache.items() if \"sae\" in k])\n",
    "\n",
    "feature_activation_df = pd.DataFrame(cache['blocks.25.hook_resid_post.hook_sae_acts_post'][0, -1, :].cpu().numpy(),\n",
    "                                     index = [f\"feature_{i}\" for i in range(sae.cfg.d_sae)],\n",
    ")\n",
    "\n",
    "\n",
    "feature_activation_df.columns = [\"heavens_and_the\"]\n",
    "feature_activation_df[\"cat_and_the\"] = cache['blocks.25.hook_resid_post.hook_sae_acts_post'][1, -1, :].cpu().numpy()\n",
    "feature_activation_df[\"diff\"]= feature_activation_df[\"heavens_and_the\"] - feature_activation_df[\"cat_and_the\"]\n",
    "\n",
    "fig = px.line(\n",
    "    feature_activation_df,\n",
    "    title=\"Feature activations for the prompt\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    ")\n",
    "\n",
    "# hide the x-ticks\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components for Feature Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:\n",
    "- Feature Activation Distribution.\n",
    "- Logit weight distribution.\n",
    "- Top 10 and bottom 10 features\n",
    "- Max Activating Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Activation Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an object to hold activations from a dataset\n",
    "from sae_lens import ActivationsStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=2048,\n",
    "    n_batches_in_buffer=8,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flatten(nested_list):\n",
    "    return [x for y in nested_list for x in y]\n",
    "\n",
    "# A very handy function Neel wrote to get context around a feature activation\n",
    "def make_token_df(tokens, len_prefix=5, len_suffix=3, model = model):\n",
    "    str_tokens = [model.to_str_tokens(t) for t in tokens]\n",
    "    unique_token = [[f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens]\n",
    "    \n",
    "    context = []\n",
    "    prompt = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for b in range(tokens.shape[0]):\n",
    "        for p in range(tokens.shape[1]):\n",
    "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
    "            if p==tokens.shape[1]-1:\n",
    "                suffix = \"\"\n",
    "            else:\n",
    "                suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
    "            prompt.append(b)\n",
    "            pos.append(p)\n",
    "            label.append(f\"{b}/{p}\")\n",
    "    # print(len(batch), len(pos), len(context), len(label))\n",
    "    return pd.DataFrame(dict(\n",
    "        str_tokens=list_flatten(str_tokens),\n",
    "        unique_token=list_flatten(unique_token),\n",
    "        context=context,\n",
    "        prompt=prompt,\n",
    "        pos=pos,\n",
    "        label=label,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "feature_list = torch.randint(0, sae.cfg.d_sae, (100,)).cpu()  # Move to CPU\n",
    "examples_found = 0\n",
    "\n",
    "total_batches = 100\n",
    "batch_size_prompts = activation_store.store_batch_size_prompts\n",
    "batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
    "pbar = tqdm(range(total_batches))\n",
    "\n",
    "all_token_dfs = []\n",
    "all_feature_acts = []\n",
    "all_fired_tokens = []\n",
    "all_reconstructions = []\n",
    "\n",
    "for i in pbar:\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "        tokens_df = make_token_df(tokens)\n",
    "        tokens_df[\"batch\"] = i\n",
    "        \n",
    "        flat_tokens = tokens.flatten()\n",
    "        \n",
    "        # Process in smaller chunks\n",
    "        chunk_size = 1024  # Adjust this value based on your GPU memory\n",
    "        for j in range(0, len(flat_tokens), chunk_size):\n",
    "            chunk = flat_tokens[j:j+chunk_size]\n",
    "            \n",
    "            _, cache = model.run_with_cache(chunk.unsqueeze(0), stop_at_layer=sae.cfg.hook_layer + 1, names_filter=[sae.cfg.hook_name])\n",
    "            sae_in = cache[sae.cfg.hook_name]\n",
    "            feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "            fired_mask = (feature_acts[:, feature_list.to('cuda:1')]).sum(dim=-1) > 0\n",
    "            fired_tokens = model.to_str_tokens(chunk[fired_mask.cpu()])\n",
    "            reconstruction = (feature_acts[fired_mask][:, feature_list.to('cuda:1')] @ sae.W_dec[feature_list.to('cuda:1')]).cpu()\n",
    "\n",
    "            # Move everything to CPU immediately\n",
    "            token_df = tokens_df.iloc[j:j+chunk_size][fired_mask.cpu().numpy()]\n",
    "            all_token_dfs.append(token_df)\n",
    "            all_feature_acts.append(feature_acts[fired_mask][:, feature_list.to('cuda:1')].cpu().numpy())\n",
    "            all_fired_tokens.extend(fired_tokens)\n",
    "            all_reconstructions.append(reconstruction.numpy())\n",
    "            \n",
    "            examples_found += len(fired_tokens)\n",
    "            \n",
    "            # Clear cache after each chunk\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    pbar.set_description(f\"Examples found: {examples_found}\")\n",
    "\n",
    "# Process results\n",
    "all_token_dfs = pd.concat(all_token_dfs, ignore_index=True)\n",
    "all_feature_acts = np.concatenate(all_feature_acts)\n",
    "all_reconstructions = np.concatenate(all_reconstructions)\n",
    "\n",
    "# Convert back to torch tensors if needed (on CPU)\n",
    "all_feature_acts = torch.from_numpy(all_feature_acts)\n",
    "all_reconstructions = torch.from_numpy(all_reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_acts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Feature Activation Histogram\n",
    "Next, we can generate the feature activation histogram (just as we saw on the dashboards above) and display the list of max-activating examples we just generated. We'll just do this for the first feature in our random set (index 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_df = pd.DataFrame(all_feature_acts.detach().cpu().numpy(), columns = [f\"feature_{i}\" for i in feature_list])\n",
    "feature_acts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = 20\n",
    "# get non-zero activations\n",
    "\n",
    "all_positive_acts = all_feature_acts[all_feature_acts[:, feature_idx] > 0][:, feature_idx].detach()\n",
    "prop_positive_activations = 100*len(all_positive_acts) / (total_batches*batch_size_tokens)\n",
    "\n",
    "px.histogram(\n",
    "    all_positive_acts.cpu(),\n",
    "    nbins=50,\n",
    "    title=f\"Histogram of positive activations - {prop_positive_activations:.3f}% of activations were positive\",\n",
    "    labels={\"value\": \"Activation\"},\n",
    "    width=800,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_activations = feature_acts_df.sort_values(f\"feature_{feature_list[0]}\", ascending=False).head(10)\n",
    "all_token_dfs.iloc[top_10_activations.index] # TODO: double check this is working correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Top 10 Logit Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we'll generate the top 10 logit weights--that is, we'll see what tokens each of the features in our set is promoting most strongly.\n",
    "\n",
    "Note it's important to fold layer norm (by default SAE Lens loads Transformers with folder layer norm but sometimes we turn preprocessing off to save GPU ram and this would affect the logit weight histograms a little bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the decoder weights {sae.W_dec.shape})\")\n",
    "print(f\"Shape of the model unembed {model.W_U.shape}\")\n",
    "projection_matrix = sae.W_dec @ model.W_U\n",
    "print(f\"Shape of the projection matrix {projection_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# then we take the top_k tokens per feature and decode them\n",
    "top_k = 10\n",
    "# let's do this for 100 random features\n",
    "_, top_k_tokens = torch.topk(projection_matrix[feature_list], top_k, dim=1)\n",
    "\n",
    "\n",
    "feature_df = pd.DataFrame(top_k_tokens.cpu().numpy(), index = [f\"feature_{i}\" for i in feature_list]).T\n",
    "feature_df.index = [f\"token_{i}\" for i in range(top_k)]\n",
    "feature_df.applymap(lambda x: model.tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bottom_k tokens per feature and decode them\n",
    "bottom_k = 10\n",
    "# We'll use the same 100 random features\n",
    "_, bottom_k_tokens = torch.topk(projection_matrix[feature_list], bottom_k, dim=1, largest=False)\n",
    "\n",
    "bottom_feature_df = pd.DataFrame(bottom_k_tokens.cpu().numpy(), index=[f\"feature_{i}\" for i in feature_list]).T\n",
    "bottom_feature_df.index = [f\"token_{i}\" for i in range(bottom_k)]\n",
    "bottom_feature_df = bottom_feature_df.applymap(lambda x: model.tokenizer.decode(x))\n",
    "\n",
    "# Display the results\n",
    "print(\"Bottom 10 logits (most inhibited tokens) for each feature:\")\n",
    "display(bottom_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
